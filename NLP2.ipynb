{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from conllu import parse\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras_contrib.layers import CRF\n",
    "from keras import callbacks\n",
    "from keras.layers import Lambda\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def check_word(word):\n",
    "    return len([c for c in word if not (c.isalnum() or c in ALLOWED_PUNCT)]) == 0\n",
    "\n",
    "def fix_word(word):\n",
    "    if UPPERCASE:\n",
    "        word = ''.join([c for c in word if (c.isalnum() or c in ALLOWED_PUNCT)])\n",
    "    else:\n",
    "        word = ''.join([c.lower() for c in word if (c.isalnum() or c in ALLOWED_PUNCT)]).lower()\n",
    "    return word\n",
    "\n",
    "def fix_punct(word):\n",
    "    word = fix_word(word)\n",
    "    for p in [P for P in TAG_PUNCT if P != ',']:\n",
    "        word = word.replace(p, '.')\n",
    "    for p in [P for P in ALLOWED_PUNCT]:\n",
    "        if p not in TAG_PUNCT:\n",
    "            word = word.replace(p, '')\n",
    "    return word\n",
    "\n",
    "def stats(data):\n",
    "    D1 = [y['form'] for x in data for y in x if y['upostag'] != 'PUNCT']\n",
    "    D2 = [y['lemma'] for x in data for y in x if y['upostag'] != 'PUNCT']\n",
    "    D3 = [[y['form'] for y in x if y['upostag'] != 'PUNCT'] for x in data]\n",
    "\n",
    "    print('Total words:', len(D1))\n",
    "    print('Total unique words:', len(set(D1)))\n",
    "    print('Total unique lemma words:', len(set(D2)))\n",
    "    print('Total chars:', len(''.join(D1)))\n",
    "    print('Total unique chars:', len(set(''.join(D1))))\n",
    "    print('Total sentences:', len(D3))\n",
    "    print('Min word len:', min([len(w) for w in D2]))\n",
    "    print('Max word len:', max([len(w) for w in D2]))\n",
    "    print('Mean word len:', int(np.mean([len(w) for w in D2])))\n",
    "    print('Min sentence len:', min([len(s) for s in D3]))\n",
    "    print('Max sentence len:', max([len(s) for s in D3]))\n",
    "    print('Mean sentence len:', int(np.mean([len(s) for s in D3])))\n",
    "\n",
    "\n",
    "def create_text_and_tagged(data, dataset_path):\n",
    "    \n",
    "    sentences = parse(data)\n",
    "    feature_sentences = []\n",
    "    tagged_sentences = []\n",
    "    stats(sentences)\n",
    "     \n",
    "    for i in range(len(sentences)):\n",
    "        \n",
    "        check = True\n",
    "        \n",
    "        sentence = sentences[i]\n",
    "        feature_sentence = []\n",
    "        tag_sentence = []\n",
    "        words = []\n",
    "        prev_punct = False\n",
    "\n",
    "        try:\n",
    "            for j in range(len(sentence)):\n",
    "                \n",
    "                word = sentence[j]\n",
    "                \n",
    "                check = check and check_word(word)\n",
    "\n",
    "                if fix_word(word['form']) != '':\n",
    "                    if word['upostag'] == 'PUNCT' and fix_word(word['form']) in ALLOWED_PUNCT:\n",
    "                        tag_sentence.append(fix_punct(word['form']))\n",
    "                        prev_punct = True\n",
    "                    elif word['upostag'] != 'PUNCT':\n",
    "                        if not prev_punct:\n",
    "                            tag_sentence.append('')\n",
    "                        feature_sentence.append(fix_word(word['form']))\n",
    "                        prev_punct = False\n",
    "                        \n",
    "            if check and (len(feature_sentence) == len(tag_sentence[1:])): \n",
    "                \n",
    "                feature_sentence = [dataset_path] + feature_sentence\n",
    "\n",
    "                feature_sentences.append(feature_sentence)\n",
    "                tagged_sentences.append(tag_sentence[1:])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return feature_sentences, tagged_sentences\n",
    "\n",
    "def prepare_dataset():\n",
    "   \n",
    "    print('Reading files')\n",
    "    \n",
    "    all_train_sentences, all_train_tags = [], []\n",
    "    all_test_sentences, all_test_tags = [], []\n",
    "    \n",
    "    dataset_id = 0\n",
    "    \n",
    "    for PATH in PATHS_TRAIN:\n",
    "        with open('UD-2.3/ud-treebanks-v2.3/' + PATH + 'train.conllu', encoding='utf-8', newline='') as f:\n",
    "            data_train = f.read()\n",
    "\n",
    "        print('Parsing UD-2.3/ud-treebanks-v2.3/' + PATH + 'train.conllu')\n",
    "        train_sentences, train_tags = create_text_and_tagged(data_train, PATH)\n",
    "        all_train_sentences += train_sentences\n",
    "        all_train_tags += train_tags\n",
    "        \n",
    "    for PATH in PATHS_TEST:\n",
    "        with open('UD-2.3/ud-treebanks-v2.3/' + PATH + 'test.conllu', encoding='utf-8', newline='') as f:\n",
    "            data_test = f.read()\n",
    "\n",
    "        print('Parsing UD-2.3/ud-treebanks-v2.3/' + PATH + 'test.conllu')\n",
    "        test_sentences, test_tags = create_text_and_tagged(data_test, PATH)\n",
    "        all_test_sentences += test_sentences\n",
    "        all_test_tags += test_tags\n",
    "\n",
    "    print('MAX_SENT_LENGTH =', MAX_SENT_LENGTH)\n",
    "    train_sentences = [elem for elem in all_train_sentences if MIN_SENT_LENGTH <= len(elem[1:]) <= MAX_SENT_LENGTH]\n",
    "    test_sentences = [elem for elem in all_test_sentences if MIN_SENT_LENGTH <= len(elem[1:]) <= MAX_SENT_LENGTH]\n",
    "    train_tags = [elem for elem in all_train_tags if MIN_SENT_LENGTH <= len(elem) <= MAX_SENT_LENGTH]\n",
    "    test_tags = [elem for elem in all_test_tags if MIN_SENT_LENGTH <= len(elem) <= MAX_SENT_LENGTH] \n",
    "\n",
    "    print(len(train_sentences), 'train samples,', len(test_sentences), 'test samples')\n",
    "\n",
    "    return train_sentences, train_tags, test_sentences, test_tags\n",
    "\n",
    "\n",
    "def get_dicts(train_sentences, train_tags):\n",
    "    \n",
    "    print('Getting dicts')\n",
    "    \n",
    "    chars = set([])\n",
    "\n",
    "    for s in train_sentences:\n",
    "        for w in s:\n",
    "            for c in w:\n",
    "                chars.add(c)\n",
    "\n",
    "    char2index = {w: i + 2 for i, w in enumerate(list(chars))}\n",
    "    char2index['CHAR_PAD'] = 0\n",
    "    char2index['CHAR_OOV'] = 1\n",
    "    \n",
    "    tags = set([])\n",
    "    \n",
    "    for s in train_tags:\n",
    "        for t in s:\n",
    "            tags.add(t)\n",
    "    for t in ALLOWED_PUNCT:\n",
    "        tags.add(t)\n",
    "\n",
    "    tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "    tag2index['PAD'] = 0  # The special value used to padding\n",
    "    \n",
    "    return char2index, tag2index\n",
    "\n",
    "def tokenize(sentences, tags, char2index, tag2index):\n",
    "\n",
    "    sentences_X, tags_y = [], []\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        s_int = []\n",
    "        for j in range(len(sentences[i]) - 1):\n",
    "            s_int2 = []\n",
    "            for c in sentences[i][j + 1]:\n",
    "                try:\n",
    "                    s_int2.append(char2index[c])\n",
    "                except KeyError:\n",
    "                    s_int2.append(char2index['CHAR_OOV'])\n",
    "                    \n",
    "            s_int.append(s_int2)\n",
    "        \n",
    "        s_int = list(pad_sequences(s_int, maxlen=MAX_WORD_LENGTH, padding='post'))\n",
    "        sentences_X.append([sentences[i][0]] + s_int)\n",
    "\n",
    "    for s in tags:\n",
    "        tags_y.append([tag2index[t] for t in s])\n",
    "\n",
    "    return sentences_X, tags_y\n",
    "\n",
    "def shuffle_and_join(sentences, tags):\n",
    "    \n",
    "    all_sentences2 = []\n",
    "    all_tags2 = []\n",
    "    \n",
    "    for PATH in set(PATHS_TRAIN + PATHS_TEST):\n",
    "        \n",
    "        indexes = [i for i in range(len(sentences)) if sentences[i][0] == PATH]\n",
    "\n",
    "        sentences2 = [sentences[i][1:] for i in indexes]\n",
    "        tags2 = [tags[i] for i in indexes]\n",
    "\n",
    "        sentences2 = [[y for x in sentences2[i:i + SENTENCES_JOINING] for y in x] for i in range(len(sentences2) - SENTENCES_JOINING + 1)]\n",
    "        tags2 = [[y for x in tags2[i:i + SENTENCES_JOINING] for y in x] for i in range(len(tags2) - SENTENCES_JOINING + 1)]\n",
    "        \n",
    "        sentences2 = pad_sequences(sentences2, maxlen=MAX_SAMPLE_LENGTH, padding='post')\n",
    "        tags2 = pad_sequences(tags2, maxlen=MAX_SAMPLE_LENGTH, padding='post')\n",
    "\n",
    "        all_sentences2 += list(sentences2)\n",
    "        all_tags2 += list(tags2)\n",
    "   \n",
    "    return np.array(all_sentences2), np.array(all_tags2)\n",
    "\n",
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)\n",
    "\n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy\n",
    "\n",
    "def f1(y_true, y_pred, tag2index, c):\n",
    "    \n",
    "    y_true_class = K.argmax(y_true, axis=-1)\n",
    "    y_pred_class = K.argmax(y_pred, axis=-1)\n",
    "    \n",
    "    true_mask = K.cast(K.equal(y_true_class, tag2index[c]), 'float')\n",
    "    pred_mask = K.cast(K.equal(y_pred_class, tag2index[c]), 'float')    \n",
    "    \n",
    "    eq = K.cast(K.equal(y_true_class, y_pred_class), 'float')\n",
    "    neq = K.cast(K.not_equal(y_true_class, y_pred_class), 'float')      \n",
    "\n",
    "    tp = K.sum(eq * true_mask)\n",
    "    fp = K.sum(neq * true_mask)\n",
    "    fn = K.sum(neq * pred_mask)\n",
    "\n",
    "    pr = tp / (tp + fp)\n",
    "    rc = tp / (tp + fn)\n",
    "    f1 = 2 * pr * rc / (pr + rc)\n",
    "\n",
    "    return f1\n",
    "\n",
    "def split_class_accuracy(char, tag2index):\n",
    "    def split_accuracy_dot(y_true, y_pred):\n",
    "        return f1(y_true, y_pred, tag2index, '.')\n",
    "\n",
    "    def split_accuracy_comma(y_true, y_pred):\n",
    "        return f1(y_true, y_pred, tag2index, ',')\n",
    "    \n",
    "    def split_accuracy_qmark(y_true, y_pred):\n",
    "        return f1(y_true, y_pred, tag2index, '?')\n",
    "    \n",
    "    def split_accuracy_emark(y_true, y_pred):\n",
    "        return f1(y_true, y_pred, tag2index, '!')\n",
    "    \n",
    "    f = {\n",
    "        '.': split_accuracy_dot,\n",
    "        ',': split_accuracy_comma,\n",
    "        '?': split_accuracy_qmark,\n",
    "        '!': split_accuracy_emark\n",
    "    }\n",
    "    \n",
    "    return f[char]\n",
    "\n",
    "\n",
    "class PlotLosses(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "                \n",
    "        draw_loss['logs'].append(logs)\n",
    "        draw_loss['x'].append(draw_loss['i'])\n",
    "        draw_loss['losses'].append(logs.get('loss'))\n",
    "        draw_loss['val_losses'].append(logs.get('val_loss'))\n",
    "        draw_loss['acc'].append(logs.get('crf_viterbi_accuracy'))\n",
    "        draw_loss['val_acc'].append(logs.get('val_crf_viterbi_accuracy'))\n",
    "        draw_loss['val_ignore'].append(logs.get('val_ignore_accuracy'))\n",
    "        draw_loss['val_split_dot'].append(logs.get('val_split_accuracy_dot'))\n",
    "        draw_loss['val_split_comma'].append(logs.get('val_split_accuracy_comma'))\n",
    "        draw_loss['val_split_qmark'].append(logs.get('val_split_accuracy_qmark'))\n",
    "        draw_loss['val_split_emark'].append(logs.get('val_split_accuracy_emark'))\n",
    "        draw_loss['i'] += 1\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(15,8))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        ax1.plot(draw_loss['x'], draw_loss['losses'], label=\"loss\")\n",
    "        ax1.plot(draw_loss['x'], draw_loss['val_losses'], label=\"val_loss\")\n",
    "        ax1.legend()\n",
    "        ax1.grid()\n",
    "        ax1.set_xlabel('Number of epochs')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Loss functions')\n",
    "\n",
    "#         ax2.plot(draw_loss['x'], draw_loss['acc'], label=\"acc\")\n",
    "#         ax2.plot(draw_loss['x'], draw_loss['val_acc'], label=\"val_acc\")\n",
    "#         ax2.plot(draw_loss['x'], draw_loss['val_ignore'], label=\"val_acc_no_pad\")\n",
    "        if '.' in TAG_PUNCT:\n",
    "            ax2.plot(draw_loss['x'], draw_loss['val_split_dot'], label=\"Period\")\n",
    "        if ',' in TAG_PUNCT:\n",
    "            ax2.plot(draw_loss['x'], draw_loss['val_split_comma'], label=\"Comma\")\n",
    "#         if '?' in TAG_PUNCT:\n",
    "#             ax2.plot(draw_loss['x'], draw_loss['val_split_qmark'], label=\"Qmark\")\n",
    "#         if '!' in TAG_PUNCT:\n",
    "#             ax2.plot(draw_loss['x'], draw_loss['val_split_emark'], label=\"Emark\")\n",
    "\n",
    "        ax2.legend()\n",
    "        ax2.grid()        \n",
    "        ax2.set_xlabel('Number of epochs')\n",
    "        ax2.set_ylabel('F1 score')\n",
    "        ax2.set_title('Punctuation restoration F1 scores')\n",
    "        \n",
    "        draw_loss['plt'] = plt\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        f.savefig('pics/' + str(datetime.timestamp(datetime.now())) + '.png')\n",
    "\n",
    "def create_model(char2index, tag2index):\n",
    "\n",
    "    print('Model creating')\n",
    "\n",
    "    input_layer = Input(shape=(MAX_SAMPLE_LENGTH,MAX_WORD_LENGTH,))\n",
    "\n",
    "    embedding_layer = Embedding(len(char2index), EMBEDDING_SIZE)(input_layer)\n",
    "\n",
    "    lstm = Bidirectional(LSTM(HID_SIZE, return_sequences=False))\n",
    "    \n",
    "    td0 = TimeDistributed(lstm, input_shape=(MAX_WORD_LENGTH, EMBEDDING_SIZE))(embedding_layer)\n",
    "    \n",
    "    lstm2 = Bidirectional(LSTM(HID_SIZE2, return_sequences=True, dropout=DROPOUT))(td0)\n",
    "\n",
    "    td1 = TimeDistributed(Dense(TD_SIZE, activation='relu'))(lstm2)\n",
    "    \n",
    "    crf = CRF(len(tag2index))\n",
    "\n",
    "    output_layer = crf(td1)\n",
    "\n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.summary()\n",
    "\n",
    "    metrics = [crf.accuracy, ignore_class_accuracy(0)]\n",
    "    \n",
    "    for c in TAG_PUNCT:\n",
    "        metrics.append(split_class_accuracy(c, tag2index))\n",
    "\n",
    "    model.compile(optimizer='adam', loss=crf.loss_function, metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def reset_draw():\n",
    "    draw_loss = {}\n",
    "    draw_loss['i'] = 0\n",
    "    draw_loss['x'] = []\n",
    "    draw_loss['losses'] = []\n",
    "    draw_loss['val_losses'] = []\n",
    "    draw_loss['acc'] = []\n",
    "    draw_loss['val_acc'] = []\n",
    "    draw_loss['val_ignore'] = []\n",
    "    draw_loss['val_split_dot'] = []\n",
    "    draw_loss['val_split_comma'] = []\n",
    "    draw_loss['val_split_qmark'] = []\n",
    "    draw_loss['val_split_emark'] = []\n",
    "    draw_loss['fig'] = plt.figure()\n",
    "    draw_loss['logs'] = []\n",
    "    return draw_loss\n",
    "\n",
    "def train_model():\n",
    "    model = create_model(char2index, tag2index)\n",
    "\n",
    "    train_sentences_X2, train_tags_y2 = shuffle_and_join(train_sentences_X, train_tags_y)\n",
    "    test_sentences_X2, test_tags_y2 = shuffle_and_join(test_sentences_X, test_tags_y)\n",
    "\n",
    "    model.fit(train_sentences_X2, to_categorical(train_tags_y2, len(tag2index)),\n",
    "          batch_size=BATCH,\n",
    "          epochs=EPOCH,\n",
    "          callbacks=[PlotLosses()],\n",
    "          validation_data=(test_sentences_X2, to_categorical(test_tags_y2, len(tag2index))))\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences\n",
    "\n",
    "\n",
    "def predict(test_sample):\n",
    "    \n",
    "    test_sample[0] = [''] + test_sample[0]\n",
    "    \n",
    "    test_samples_X, _ = tokenize(test_sample, [], char2index, tag2index)\n",
    "    \n",
    "    test_samples_X[0] = test_samples_X[0][1:]\n",
    "    \n",
    "    test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_SAMPLE_LENGTH, padding='post')\n",
    "        \n",
    "    predictions = model.predict(test_samples_X)\n",
    "\n",
    "    result = logits_to_tokens(predictions, {i: t for t, i in tag2index.items()})\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def transform(sentence_features):\n",
    "    tags = predict(sentence_features)\n",
    "    return ' '.join([' '.join(sentence_features[i][j+1] + tags[i][j] for j in range(len(sentence_features[i])-1)) for i in range(len(sentence_features))])\n",
    "\n",
    "def validate():\n",
    "    r = random.randint(0, len(test_sentences) - SENTENCES_JOINING)\n",
    "    ind = list(range(r, r + SENTENCES_JOINING))\n",
    "    sentence_features = [test_sentences[q][1:] for q in ind]\n",
    "    sentence_tags = [test_tags[q] for q in ind]\n",
    "    sentence_features = [[y for x in sentence_features for y in x]]\n",
    "\n",
    "    pred = transform(sentence_features)\n",
    "\n",
    "    print(sentence_features)\n",
    "    print(sentence_tags)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'ru': 'UD_Russian-SynTagRus/ru_syntagrus-ud-',\n",
    "    'ru-small': 'UD_Russian-GSD/ru_gsd-ud-',\n",
    "    'en': 'UD_English-GUM/en_gum-ud-',\n",
    "    'uk': 'UD_Ukrainian-IU/uk_iu-ud-',\n",
    "    'be': 'UD_Belarusian-HSE/be_hse-ud-',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PATHS_TRAIN = [paths[key] for key in ['ru']]\n",
    "PATHS_TEST = [paths[key] for key in ['ru']]\n",
    "\n",
    "ALLOWED_PUNCT = ['.',',','!','?','?!','!?','..','...']\n",
    "TAG_PUNCT = ['.', ',', '?', '!']\n",
    "NGRAM = 2\n",
    "\n",
    "DO_SHUFFLE = False\n",
    "UPPERCASE = False\n",
    "\n",
    "MIN_SENT_LENGTH = -1\n",
    "MAX_SENT_LENGTH = 40\n",
    "MAX_WORD_LENGTH = 30\n",
    "\n",
    "random.seed(0)\n",
    "train_sentences, train_tags, test_sentences, test_tags = prepare_dataset()\n",
    "char2index, tag2index = get_dicts([i[1:] for i in train_sentences], train_tags)\n",
    "train_sentences_X, train_tags_y = tokenize(train_sentences, train_tags, char2index, tag2index)\n",
    "test_sentences_X, test_tags_y = tokenize(test_sentences, test_tags, char2index, tag2index)\n",
    "\n",
    "SENTENCES_JOINING = 3\n",
    "DROPOUT = 0.7\n",
    "EMBEDDING_SIZE = 100\n",
    "HID_SIZE = 200\n",
    "HID_SIZE2 = 500\n",
    "TD_SIZE = 10\n",
    "BATCH = 50\n",
    "EPOCH = 100\n",
    "\n",
    "if not MAX_SENT_LENGTH:\n",
    "    MAX_SENT_LENGTH = len(max(train_sentences, key=len))\n",
    "\n",
    "MAX_SAMPLE_LENGTH = MAX_SENT_LENGTH * SENTENCES_JOINING\n",
    "\n",
    "draw_loss = reset_draw()\n",
    "\n",
    "model = train_model()\n",
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 50\n",
    "r = random.randint(0, len(test_sentences) - size)\n",
    "ind = list(range(r, r + size))\n",
    "sentence_features = [test_sentences[q][1:] for q in ind]\n",
    "sentence_tags = [test_tags[q] for q in ind]\n",
    "sentence_features = [[y for x in sentence_features for y in x]]\n",
    "\n",
    "counters = [{i:0 for i in TAG_PUNCT} for j in range(len(sentence_features[0]))]\n",
    "for i in range(len(counters)):\n",
    "    counters[i][''] = 0\n",
    "    counters[i]['PAD'] = 0\n",
    "    \n",
    "preds = [predict([sentence_features[0][r:r+MAX_SAMPLE_LENGTH]]) for r in range(len(sentence_features[0])-MAX_SAMPLE_LENGTH+1)]\n",
    "for i in range(len(preds)):\n",
    "    for j in range(len(preds[i][0])):\n",
    "        counters[i+j][preds[i][0][j]] += 1\n",
    "\n",
    "best = [max(i, key=i.get) for i in counters]\n",
    "\n",
    "# print(sentence_features)\n",
    "# print(sentence_tags)\n",
    "# print(counters)\n",
    "# print(best)\n",
    "# print(preds)\n",
    "print(' '.join([''.join(i) for i in list(zip(sentence_features[0], best))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2char = {char2index[i]:i for i in char2index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sentences_X2, test_tags_y2 = shuffle_and_join(test_sentences_X, test_tags_y)\n",
    "[''.join([index2char[j] for j in i]).replace('CHAR_PAD','') for i in test_sentences_X2[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UD-2.3/ud-treebanks-v2.3/' + paths['en'] + 'train.conllu', encoding='utf-8', newline='') as f:\n",
    "    data = parse(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = [y['form'] for x in data for y in x if y['upostag'] != 'PUNCT']\n",
    "D2 = [y['lemma'] for x in data for y in x if y['upostag'] != 'PUNCT']\n",
    "D3 = [[y['form'] for y in x if y['upostag'] != 'PUNCT'] for x in data]\n",
    "\n",
    "print('Total words:', len(D1))\n",
    "print('Total unique words:', len(set(D1)))\n",
    "print('Total unique lemma words:', len(set(D2)))\n",
    "print('Total chars:', len(''.join(D1)))\n",
    "print('Total unique chars:', len(set(''.join(D1))))\n",
    "print('Total sentences:', len(D3))\n",
    "print('Min word len:', min([len(w) for w in D2]))\n",
    "print('Max word len:', max([len(w) for w in D2]))\n",
    "print('Mean word len:', int(np.mean([len(w) for w in D2])))\n",
    "print('Min sentence len:', min([len(s) for s in D3]))\n",
    "print('Max sentence len:', max([len(s) for s in D3]))\n",
    "print('Mean sentence len:', int(np.mean([len(s) for s in D3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist([len(w) for w in D3], bins=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
